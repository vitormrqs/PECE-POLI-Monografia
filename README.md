# Monografia

Trabalho de conclusÃ£o de curso realizado no decorrer do ano de 2023, orientado pela professora:
* ProfÂª. MSc. Ana Claudia Rossi

A monografia "Roteiro para MigraÃ§Ã£o de Dados de CRM na Ã¡rea financeira: Integrando a Arquitetura Data Mesh com a EstratÃ©gia de ClassificaÃ§Ã£o MedalhÃ£o" consiste em trabalhar com dados utilizando arquitetura Data Mesh, classificaÃ§Ã£o medalhÃ£o e arquitetura de referÃªncia NBDRA (NIST Big Data Reference Architecture)
<!-- O projeto consiste em responder 3 questÃµes de negÃ³cios por meio de anÃ¡lise de um BI construÃ­do atravÃ©s de ETL. Para tal, deve-se ingerir dados do repostÃ³rio pÃºblico [Speedtest by Ookla Global Fixed and Mobile Network Performance Map Tiles](https://github.com/teamookla/ookla-open-data#speedtest-by-ookla-global-fixed-and-mobile-network-performance-map-tiles), tratÃ¡-los, automatizÃ¡-los e disponibilizÃ¡-los em camada de BI para anÃ¡lise.
Foi utilizada a arquitetura medalhÃ£o para o projeto, sendo os dados divididos em camada raw (camada dos dados iniciais), trusted (camada tratada) e delivery (camada com modelagem aplicada). -->

## ğŸš€ Etapas do projeto
<!-- 
Foi definido os seguintes passos:
1. IngestÃ£o de Dados
2. Processamento de Dados
3. Armazenamento de Dados
4. OrquestraÃ§Ã£o de ferramentas
5. Qualidade dos dados
6. VisualizaÃ§Ã£o -->

## ğŸ“‹ Arquitetura do projeto

<!-- O arquivo para arquitetura do projeto estÃ¡ localizado em `arquitetura\Arquitetura Projeto Integrador.drawio`.
![Arquitetura_Fluxo](/arquitetura/Arquitetura.png) -->

## ğŸ”§ Projeto

<!-- ### 1. Dados

Foram utilizados os dados mais recentes do repositÃ³rio Okla, sendo eles: 2022 e 2023 (1Âº quadrimestre).
Os dados estÃ£o disponÃ­veis por redes fixas e mÃ³veis, por ano e por extensÃ£o de arquivo (parquet / shapefile).

### 2. Fluxo de leitura

AtravÃ©s do repositÃ³rio inicial, foi lido os arquivos em script construÃ­do para ingestÃ£o.
> O cÃ³digo que lÃª os arquivos iniciais e realiza os primeiros tratamentos estÃ¡ em: `ETL/gluejobs_trusted.py`.

ApÃ³s inserido os dados na camada raw e feito os tratamentos iniciais como a revisÃ£o de dimensÃµes de:
1. Completude
2. Validade
3. Integridade
4. Exclusividade

os dados sÃ£o validados atravÃ©s do script `validacao\gluejobs_validacoes.py`.

Finalmente, sÃ£o avaliados e filtrados para serem disponibilizados na camada delivery. Nessa etapa, Ã© criada a tabela fato e suas dimenÃµes como serÃ¡ explicada no capÃ­tulo a seguir. Os dados sÃ£o construÃ­dos atravÃ©s do script `ETL\gluejobs_delivery.py`.

### 3. Modelagem Star Schema

A tabela fato contÃ©m cada evento realizado de teste e possui dimensÃµes de: `dim_Localizacao`, `dim_Tipo`, `dim_Periodo`. Foi escolhido esse modelo a fim de disponibilizarmos informaÃ§Ãµes visuais dos dados.

![Star Schema](/modelagem.jpeg)

| Tabela          | Atributos            | DescriÃ§Ã£o                                                            |
|-----------------|----------------------|----------------------------------------------------------------------|
| Fato            | FK_Id_Tipo           | Chave estrangeira para a tabela dim_Tipo (Id_Tipo)                   |
| Fato            | FK_Id_Localizacao    | Chave estrangeira para a tabela dim_Localizacao (Id_Geo)             |
| Fato            | FK_Id_Periodo        | Chave estrangeira para a tabela dim_Periodo (Id_Temp)                |
| Fato            | avg_d_kbps           | Velocidade mÃ©dia de download em kilobits por segundo                 |
| Fato            | avg_u_kbps           | Velocidade mÃ©dia de upload em kilobits por segundo                   |
| Fato            | avg_lat_ms           | LatÃªncia mÃ©dia em milissegundos                                      |
| Fato            | tests                | NÃºmero de testes realizados no bloco                                 |
| Fato            | devices              | NÃºmero de dispositivos exclusivos que contribuem com testes no bloco |
| dim_Tipo        | PK_Id_Tipo           | Chave primÃ¡ria da tabela dim_Tipo                                    |
| dim_Tipo        | tipo                 | Tipo de dispositivo (mÃ³vel ou fixo)                                  |
| dim_Localizacao | PK_Id_Geo            | Chave primÃ¡ria da tabela dim_Localizacao                             |
| dim_Localizacao | quadkey              | Quadrante que representa o espaÃ§o/bloco dos testes                   |
| dim_Localizacao | latitude             | Latitude da localizaÃ§Ã£o dos testes                                   |
| dim_Localizacao | longitude            | Longitude da localizaÃ§Ã£o dos testes                                  |
| dim_Periodo     | PK_Id_Temp           | Chave primÃ¡ria da tabela dim_Periodo                                 |
| dim_Periodo     | year                 | Ano da realizaÃ§Ã£o dos testes                                         |
| dim_Periodo     | quad                 | Quadrimestre da realizaÃ§Ã£o dos testes                                |

Os cÃ³digos utilizados para criaÃ§Ã£o da tabela estÃ£o diponÃ­veis em `ETL\gluejobs_delivery.py`.

### 4. *Dashboards*

Por meio do Power BI foi possÃ­vel acessar as tabelas montadas pelo Crawler da AWS e criar o relatÃ³rio que Ã© utilizado na Ã¡rea de negÃ³cios.

![Dashboard](/relatorio/dashboard.jpeg)

O arquivo estÃ¡ localizado em `relatorio\ProjetoIntegrador.pbix`. -->

## ğŸ› ï¸ ConstruÃ­do com

* [AWS](https://us-east-1.console.aws.amazon.com/) - Plataforma utilizada para construÃ§Ã£o do pipeline dos dados
* [Draw.io](https://app.diagrams.net/) - Programa para desenvolvimento do modelo estrela;
* [Power BI](https://powerbi.microsoft.com/pt-br/) - Programa para desenvolvimento de relatÃ³rios; 

## âœ’ï¸ Autor

* [Vitor Marques](https://github.com/vitormrqs)